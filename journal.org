# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages
#+TITLE:       Laboratory Notebook for a Multi-Threaded Version of Quicksort
#+AUTHOR:      Arnaud Legrand
#+LANGUAGE:    en
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n)

* Project Overview
This project aims at providing an efficient multi-threaded
implementation of the QuickSort algorithm on multi-core machines. This
document contains my attempts to evaluate the performance of an
implementation of such code.
* General Organization
** src/
This directory comprises the parallel implementation and a standard
Makefile to compile it.
** data/
This is where raw experimental data should go. Each directory entry
comprises a set of experiments and the directory name is based on the
machine name and on the date. For example:
#+begin_src sh :results output :exports both
echo mkdir data/`hostname`_`date +%F`
#+end_src

#+RESULTS:
: mkdir data/sama_2014-10-13

* Typical usage
** Compilation
A simple makefile with various compilation options is provided in the
src/ directory. Compilation is thus done by running the following command:
#+begin_src sh :results output :exports both
make -C src/
#+end_src

#+RESULTS:
: make: Entering directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  -c -o parallelQuicksort.o parallelQuicksort.c
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  parallelQuicksort.o  -o parallelQuicksort
: make: Leaving directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'

** Running the code
The code is quite simple at the moment and can be run in the following way:
#+begin_src
./src/parallelQuicksort [1000000]
#+end_src
When run, the code executes initializes an array of the size given in
argument (1000000 by default) with random integer values and sorts it
using:
1. a custom sequential implementation;
2. a custom parallel implementation;
3. the libc qsort function.
Times are reported in seconds.
* Experimental Reports
** 2014-10-13
*** Initial code design
- I obtained an initial implementation from
  http://sc12.supercomputing.org/hpceducator/PythonForParallelism/codes/parallelQuicksort.c.
  According to the header, the original author is Joshua Stough from
  Washington and Lee University. I hope he will not mind that I reuse
  this piece of code for educational purposes.
- Here is a typical first execution on my laptop (an Intel(R) Core(TM)
  i7 running a Debian with Linux 3.14.15):
  #+begin_src sh :results output :exports both
    ./src/quicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.231571 sec.
  : at loc 506315, 5.068226e-01 < 5.068269e-01
  : Oops, lyst did not get sorted by parallelQuicksort.
  : Parallel quicksort took: 0.161259 sec.
  : Built-in qsort took: 0.241568 sec.

  Sweet, in my first attempt, it looks like this parallel version is
  indeed running faster than then sequential one. I have to say this
  warning message is stressing me a bit though.
- On smaller instances, the code would segfault. So I reindented the
  code and thanks to valgrind and gdb, I could find what was wrong. I
  also renamed the file so that compiling is more convenient. This
  fixed the previous warning message so now everything seems fine:
  #+begin_src sh :results output :exports both
    ./src/parallelQuicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.239347 sec.
  : Parallel quicksort took: 0.176365 sec.
  : Built-in quicksort took: 0.244716 sec.

*** First series of experiments
Let's try to see how the three algorithms behave when changing the
array size. Since one measurement is not enough, I run the code 5
times in a row.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/sama_2014-10-13/measurements_03:47.txt][output]].

*** A simple plot with R
Here is a simple script to parse the results:
#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor.pl
  use strict;

  my($line);
  my($size);

  print "Size, Type, Time\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      }
      if($line =~/^(.*) quicksort.*: ([\d\.]*) sec.$/) {
          print "$size, \"$1\", $2\n" ;
          next;
      }
  }
#+end_src

I can then simply parse my data with the following command:

#+begin_src sh :results output :exports both
perl scripts/csv_quicksort_extractor.pl < data/sama_2014-10-13/measurements_03\:47.txt > data/sama_2014-10-13/measurements_03\:47.csv
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/sama_2014-10-13/measurements_03:47.png :exports both :width 600 :height 400 :session
  df <- read.csv("data/sama_2014-10-13/measurements_03:47.csv",header=T)
  plot(df$Size,df$Time,col=c("red","blue","green")[df$Type])
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47.png]]

Well, this is not particularly nice and some may not know/like R.
*** A simple plot with gnuplot
So let's try to parse in an other way and use gnuplot:

#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor2.pl
  use strict;

  my($line);
  my($size);
  my($seq,$par,$libc);
  print "Size, Seq, Par, Libc\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      }
      if($line =~/^Sequential quicksort.*: ([\d\.]*) sec.$/) {
          $seq=$1; next;
      }
      if($line =~/^Parallel quicksort.*: ([\d\.]*) sec.$/) {
          $par=$1; next;
      }
      if($line =~/^Built-in quicksort.*: ([\d\.]*) sec.$/) {
          $libc=$1;
          print "$size, $seq, $pqr, $libc\n";
          next;
      }
  }
#+end_src

#+begin_src sh :results output raw :exports both
  FILENAME="data/sama_2014-10-13/measurements_03:47"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47_wide.png]]

Well, I'm not sure it is nicer but I have lines. A first crude
analysis seems to reveal the the parallel version is worth it for
arrays larger than 400000.
** 2015-08-19
This entry was writen by Juliano Bittencorut
<juliano.bittencourt@gmail.com> as part of an exercise for the 2015/2
SMPE class at UFRGS/Brazil.

*** Initial code install
- I obtained the code by cloning Arnaud Legrand's git repo at github in my local machine,
  as part of the SMPE/UFRGS course exercise.
  https://github.com/alegrand/M2R-ParallelQuicksort
- I'll run the experiments on my laptop, a Late 2008 MacBook
  Pro. System Overview

   #+BEGIN_SRC sh :results output
   uname -a
   #+End_SRC

   #+RESULTS:
   : Darwin Julianos-MacBook-Pro.local 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64

   #+BEGIN_SRC sh :results output
   sw_vers
   #+END_SRC

   #+RESULTS:
   : ProductName:	Mac OS X
   : ProductVersion:	10.10.5
   : BuildVersion:	14F27

   #+BEGIN_SRC sh :results output
     system_profiler SPHardwareDataType
   #+END_SRC

   #+RESULTS:
   #+begin_example
   Hardware:

       Hardware Overview:

         Model Name: MacBook Pro
         Model Identifier: MacBookPro5,1
         Processor Name: Intel Core 2 Duo
         Processor Speed: 2,4 GHz
         Number of Processors: 1
         Total Number of Cores: 2
         L2 Cache: 3 MB
         Memory: 8 GB
         Bus Speed: 1,07 GHz
         Boot ROM Version: MBP51.007E.B00
         SMC Version (system): 1.41f2
         Serial Number (system): W89203HD8Q1
         Hardware UUID: 8406B003-DA62-5F30-BF66-69D863570F45
         Sudden Motion Sensor:
             State: Enabled

#+end_example


- Since I've recently reinstalled my OS, I needed to reinstall Xcode 5
  and Xcode command line tools:

  #+BEGIN_SRC
  xcode-select --install
  #+END_SRC


- First step was to take to compile the code in my machine. But when I
  ran make got a complaining about librt.

   #+BEGIN_SRC sh :results output :export both
     make -C src
   #+End_SRC

   #+RESULTS:
   : clang:   warning: argument unused during compilation: '-pthread'
   : ld:      library not found for -lrt
   : clang: error: linker command failed with exit code 1 (use -v to see invocation)
   : make: *** [parallelQuicksort] Error 1

- Removed the librt from CFLAGS in the Makefile and tried to compile
  again. Apparently everything went smootly. Hope things don't start segfaulting.

  #+BEGIN_SRC sh :results output
    make -C src
  #+END_SRC

  #+RESULTS:
  : cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread  -std=c99 parallelQuicksort.o  -o parallelQuicksort

- I've tried to run the code. Aparently things went ok, with the
  parallel verions running slithly faster. Considering that I have a 2
  core machine, I didn't expected a lot of improvment.

   #+begin_src sh :results output :exports both
     ./src/parallelQuicksort
   #+end_src

   #+RESULTS:
   : Sequential quicksort took: 0.437233 sec.
   : Parallel quicksort took: 0.494957 sec.
   : Built-in quicksort took: 0.559151 sec.

*** First set os experiments
My intention is to replicate Arnaud Legrand experiment on my
laptop. The experiments details can be found on the 2014-10-13 entry
on this journal.

My initial hyphotesis is that I'm going to find results that are much
slower than Arnaund experiment. I also expect to see a slighly better
perfomance of the parallel version over the procedural one.

#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

The output file can be viewed [[file:data/Julianos-MacBook-Pro.local_2015-08-19/measurements_20:12.txt][here]].

I used Arnaund script to parse the data.

#+BEGIN_SRC sh :results output
perl scripts/csv_quicksort_extractor.pl < data/Julianos-MacBook-Pro.local_2015-08-19/measurements_20:12.txt > data/Julianos-MacBook-Pro.local_2015-08-19/measurements_20:12.csv
#+END_SRC

#+RESULTS:

To plot the data I've changed the original script in order to get a
niver plot with ggplot2

#+begin_src R :results output graphics :file data/Julianos-MacBook-Pro.local_2015-08-19/measurements_20:12.png :exports both :width 600 :height 400 :session
  library(ggplot2)
  df <- read.csv("data/Julianos-MacBook-Pro.local_2015-08-19/measurements_20:12.csv",header=T)
  plot1 <- ggplot(df,aes(x=Size,y=Time,shape=Type)) + geom_point(aes(color=Type,show_guide=FALSE))
  print(plot1)
#+end_src

#+RESULTS:
[[file:data/Julianos-MacBook-Pro.local_2015-08-19/measurements_20:12.png]]

It's interesting o to observe that in my system the parallel version
is always slower. Sometimes it's way slower than the sequential or
buildin version. That diffrent from what I've expected. Since I'm
running this on my personal laptop, with other applications running
that might be using the CPU, it's possible that the overhead caused by
the parallel version impacted the results.


